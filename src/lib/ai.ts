/**
 * OpenAI LangChain æœåŠ¡æŠ½è±¡å±‚
 * ä½¿ç”¨ LangChain.js è§„èŒƒï¼Œä»…æ”¯æŒ OpenAI
 * Anthropic è¯·ä½¿ç”¨ @/services/ai/anthropic
 */

import { ChatOpenAI } from '@langchain/openai';
import { ChatPromptTemplate } from '@langchain/core/prompts';
import { StringOutputParser } from '@langchain/core/output_parsers';
import type { Runnable } from '@langchain/core/runnables';

// å¯¼å‡º LangChain æ ¸å¿ƒç±»å‹å’Œå·¥å…·ï¼Œæ–¹ä¾¿å…¶ä»–æ¨¡å—ä½¿ç”¨
export { ChatPromptTemplate } from '@langchain/core/prompts';
export { StringOutputParser } from '@langchain/core/output_parsers';

/**
 * OpenAI æ¨¡å‹é…ç½®æ¥å£
 */
export interface AIModelConfig {
  /** æ¨¡å‹åç§° */
  model?: string;
  /** æ¸©åº¦å‚æ•° */
  temperature?: number;
  /** æœ€å¤§ token æ•° */
  maxTokens?: number;
}

/**
 * é»˜è®¤æ¨¡å‹é…ç½®
 */
const DEFAULT_MODEL_CONFIG: AIModelConfig = {
  model: 'gpt-4o-mini',
  temperature: 0.7,
  maxTokens: 2000,
};

export const createOpenAIModel = (config: AIModelConfig = {}): ChatOpenAI => {
  const mergedConfig = { ...DEFAULT_MODEL_CONFIG, ...config };
  return new ChatOpenAI({
    apiKey: process.env.OPENAI_AUTH_TOKEN,
    openAIApiKey: process.env.OPENAI_AUTH_TOKEN,
    configuration: {
      baseURL: process.env.OPENAI_BASE_URL_PROXY,
    },
    streaming: true,
    model: mergedConfig.model,
    temperature: mergedConfig.temperature,
    maxTokens: mergedConfig.maxTokens,
  });
};

/**
 * åˆ›å»º StringOutputParser å®ä¾‹
 * @returns StringOutputParser å®ä¾‹
 */
export const createStringOutputParser = (): StringOutputParser => {
  return new StringOutputParser();
};

/**
 * å°† LangChain æµå¼å“åº”è½¬æ¢ä¸º ReadableStream
 * ä½¿ç”¨ start æ–¹æ³•æŒç»­è¯»å–å¹¶æ¨é€æ•°æ®åˆ°å®¢æˆ·ç«¯
 * @param streamIterable LangChain æµå¼å“åº”ï¼ˆAsyncIterable<string>ï¼‰
 * @returns ReadableStream
 */
export const convertLangChainStreamToReadableStream = (
  streamIterable: AsyncIterable<string>,
): ReadableStream<Uint8Array> => {
  const encoder = new TextEncoder();
  const iterator = streamIterable[Symbol.asyncIterator]();

  return new ReadableStream<Uint8Array>({
    async start(controller) {
      try {
        let chunkCount = 0;
        console.log('ğŸ”„ LangChain æµå¼å“åº”å¼€å§‹è¯»å–...');
        
        while (true) {
          const { done, value } = await iterator.next();
          
          if (done) {
            console.log(`âœ… LangChain æµå¼å“åº”å®Œæˆï¼Œå…±å¤„ç† ${chunkCount} ä¸ªæ•°æ®å—`);
            controller.close();
            break;
          }
          
          if (value) {
            chunkCount++;
            const encoded = encoder.encode(value);
            controller.enqueue(encoded);
            
            // å‰å‡ ä¸ªå—è¾“å‡ºæ—¥å¿—
            if (chunkCount <= 3 || chunkCount % 20 === 0) {
              console.log(`ğŸ“¤ LangChain ç¬¬ ${chunkCount} ä¸ªæ•°æ®å—ï¼Œé•¿åº¦: ${value.length}ï¼Œå†…å®¹é¢„è§ˆ: ${value.substring(0, 50)}...`);
            }
          } else {
            console.warn(`âš ï¸ LangChain ç¬¬ ${chunkCount + 1} æ¬¡è¯»å–åˆ°ç©ºå€¼`);
          }
        }
      } catch (error) {
        console.error('âŒ LangChain æµå¼å“åº”é”™è¯¯:', error);
        if (error instanceof Error) {
          console.error('é”™è¯¯å †æ ˆ:', error.stack);
        }
        const errorMessage = error instanceof Error ? error.message : 'AIå¤„ç†å¤±è´¥';
        controller.error(new Error(errorMessage));
      }
    },
    cancel() {
      console.log('âš ï¸ LangChain æµå¼å“åº”è¢«å–æ¶ˆ');
      // å½“å®¢æˆ·ç«¯å–æ¶ˆæ—¶æ¸…ç†èµ„æº
      iterator.return?.();
    },
  });
};

/**
 * ä½¿ç”¨ LCEL é“¾æ‰§è¡Œæµå¼å“åº”
 * @param chain LangChain Runnable é“¾
 * @param input è¾“å…¥å‚æ•°
 * @returns ReadableStream
 */
export const streamFromChain = async <T extends Record<string, unknown>>(
  chain: Runnable<T, string>,
  input: T,
): Promise<ReadableStream> => {
  const stream = await chain.stream(input);
  return convertLangChainStreamToReadableStream(stream);
};

/**
 * åˆ›å»º OpenAI AI å¤„ç†é“¾
 * ä½¿ç”¨ LCEL è§„èŒƒï¼šprompt.pipe(model).pipe(outputParser)
 * æ³¨æ„ï¼šAnthropic è¯·ä½¿ç”¨ @/services/ai/anthropic
 * @param prompt ChatPromptTemplate æç¤ºè¯æ¨¡æ¿
 * @param config æ¨¡å‹é…ç½®
 * @returns Runnable é“¾
 */
export function createAIChain<T extends Record<string, unknown>>(
  prompt: ChatPromptTemplate,
  config: AIModelConfig = {}
): Runnable<T, string> {
  const model = createOpenAIModel(config);
  const outputParser = createStringOutputParser();
  return prompt.pipe(model).pipe(outputParser) as Runnable<T, string>;
}