/**
 * OpenAI LangChain æœåŠ¡æŠ½è±¡å±‚
 * ä½¿ç”¨ LangChain.js è§„èŒƒï¼Œä»…æ”¯æŒ OpenAI
 * Anthropic è¯·ä½¿ç”¨ @/services/ai/anthropic
 */

import { ChatOpenAI } from '@langchain/openai';
import { ChatPromptTemplate } from '@langchain/core/prompts';
import { StringOutputParser } from '@langchain/core/output_parsers';
import type { Runnable } from '@langchain/core/runnables';
import { RunnableLambda } from '@langchain/core/runnables';

// å¯¼å‡º LangChain æ ¸å¿ƒç±»å‹å’Œå·¥å…·ï¼Œæ–¹ä¾¿å…¶ä»–æ¨¡å—ä½¿ç”¨
export { ChatPromptTemplate } from '@langchain/core/prompts';
export { StringOutputParser } from '@langchain/core/output_parsers';

/**
 * OpenAI æ¨¡å‹é…ç½®æ¥å£
 */
export interface AIModelConfig {
  /** æ¨¡å‹åç§° */
  model?: string;
  /** æ¸©åº¦å‚æ•° */
  temperature?: number;
  /** æœ€å¤§ token æ•° */
  maxTokens?: number;
}

/**
 * é»˜è®¤æ¨¡å‹é…ç½®
 */
const DEFAULT_MODEL_CONFIG: AIModelConfig = {
  model: process.env.SILICONFLOW_FREE_QWEN,
  temperature: 0.7,
  maxTokens: 2000,
};

export const createOpenAIModel = (config: AIModelConfig = {}): ChatOpenAI => {
  const mergedConfig = { ...DEFAULT_MODEL_CONFIG, ...config };
  return new ChatOpenAI({
    apiKey: process.env.SILICONFLOW_API_KEY,
    configuration: {
      baseURL: process.env.SILICONFLOW_BASE_URL,
    },
    streaming: true,
    model: mergedConfig.model,
    temperature: mergedConfig.temperature,
    maxTokens: mergedConfig.maxTokens,
  });
};

/**
 * åˆ›å»º StringOutputParser å®ä¾‹
 * @returns StringOutputParser å®ä¾‹
 */
export const createStringOutputParser = (): StringOutputParser => {
  return new StringOutputParser();
};

/**
 * å°† LangChain æµå¼å“åº”è½¬æ¢ä¸º ReadableStream
 * ä½¿ç”¨ start æ–¹æ³•æŒç»­è¯»å–å¹¶æ¨é€æ•°æ®åˆ°å®¢æˆ·ç«¯
 * @param streamIterable LangChain æµå¼å“åº”ï¼ˆAsyncIterable<string>ï¼‰
 * @returns ReadableStream
 */
export const convertLangChainStreamToReadableStream = (
  streamIterable: AsyncIterable<string>,
): ReadableStream<Uint8Array> => {
  const encoder = new TextEncoder();
  const iterator = streamIterable[Symbol.asyncIterator]();

  return new ReadableStream<Uint8Array>({
    async start(controller) {
      try {
        let chunkCount = 0;
        console.log('ğŸ”„ LangChain æµå¼å“åº”å¼€å§‹è¯»å–...');
        
        while (true) {
          const { done, value } = await iterator.next();
          
          if (done) {
            console.log(`âœ… LangChain æµå¼å“åº”å®Œæˆï¼Œå…±å¤„ç† ${chunkCount} ä¸ªæ•°æ®å—`);
            controller.close();
            break;
          }
          
          if (value) {
            chunkCount++;
            const encoded = encoder.encode(value);
            controller.enqueue(encoded);
            
            // å‰å‡ ä¸ªå—è¾“å‡ºæ—¥å¿—
            if (chunkCount <= 3 || chunkCount % 20 === 0) {
              console.log(`ğŸ“¤ LangChain ç¬¬ ${chunkCount} ä¸ªæ•°æ®å—ï¼Œé•¿åº¦: ${value.length}ï¼Œå†…å®¹é¢„è§ˆ: ${value.substring(0, 50)}...`);
            }
          } else {
            console.warn(`âš ï¸ LangChain ç¬¬ ${chunkCount + 1} æ¬¡è¯»å–åˆ°ç©ºå€¼`);
          }
        }
      } catch (error) {
        console.error('âŒ LangChain æµå¼å“åº”é”™è¯¯:', error);
        if (error instanceof Error) {
          console.error('é”™è¯¯å †æ ˆ:', error.stack);
        }
        const errorMessage = error instanceof Error ? error.message : 'AIå¤„ç†å¤±è´¥';
        controller.error(new Error(errorMessage));
      }
    },
    cancel() {
      console.log('âš ï¸ LangChain æµå¼å“åº”è¢«å–æ¶ˆ');
      // å½“å®¢æˆ·ç«¯å–æ¶ˆæ—¶æ¸…ç†èµ„æº
      iterator.return?.();
    },
  });
};

/**
 * ä½¿ç”¨ LCEL é“¾æ‰§è¡Œæµå¼å“åº”
 * @param chain LangChain Runnable é“¾
 * @param input è¾“å…¥å‚æ•°
 * @returns ReadableStream
 */
export const streamFromChain = async <T extends Record<string, unknown>>(
  chain: Runnable<T, string>,
  input: T,
): Promise<ReadableStream> => {
  const stream = await chain.stream(input);
  return convertLangChainStreamToReadableStream(stream);
};

/**
 * ä»æ¶ˆæ¯å—ä¸­æå–æ–‡æœ¬å†…å®¹
 * å¤„ç† LangChain çš„æ¶ˆæ¯å¯¹è±¡ï¼Œæå–æ–‡æœ¬å†…å®¹ï¼Œå¿½ç•¥å…ƒæ•°æ®
 */
const extractTextFromMessage = (message: unknown): string => {
  if (typeof message === 'string') {
    return message;
  }
  
  if (message && typeof message === 'object') {
    // å¤„ç† LangChain çš„æ¶ˆæ¯å—æ ¼å¼
    // AIMessageChunk æˆ–ç±»ä¼¼å¯¹è±¡
    if ('content' in message) {
      const content = message.content;
      if (typeof content === 'string') {
        return content;
      }
      // å¦‚æœ content æ˜¯æ•°ç»„ï¼ˆå¤šéƒ¨åˆ†æ¶ˆæ¯ï¼‰ï¼Œæå–æ–‡æœ¬éƒ¨åˆ†
      if (Array.isArray(content)) {
        return content
          .map((part) => {
            if (typeof part === 'string') return part;
            if (part && typeof part === 'object' && 'text' in part && typeof part.text === 'string') {
              return part.text;
            }
            return '';
          })
          .join('');
      }
    }
    
    // å¤„ç† lc_kwargs æ ¼å¼ï¼ˆLangChain å†…éƒ¨æ ¼å¼ï¼‰
    if ('lc_kwargs' in message && typeof message.lc_kwargs === 'object') {
      const kwargs = message.lc_kwargs as Record<string, unknown>;
      if (kwargs.content) {
        if (typeof kwargs.content === 'string') {
          return kwargs.content;
        }
        if (Array.isArray(kwargs.content)) {
          return kwargs.content
            .map((part) => {
              if (typeof part === 'string') return part;
              if (part && typeof part === 'object' && 'text' in part && typeof part.text === 'string') {
                return part.text;
              }
              return '';
            })
            .join('');
        }
      }
    }
  }
  
  return '';
};

/**
 * åˆ›å»ºæ–‡æœ¬æå–å™¨ï¼ˆç”¨äºæµå¼å“åº”ï¼‰
 * ä»æ¨¡å‹çš„æ¶ˆæ¯å—ä¸­æå–æ–‡æœ¬ï¼Œå¿½ç•¥å…ƒæ•°æ®
 */
const createTextExtractor = () => {
  return new RunnableLambda({
    func: async (input: unknown) => {
      return extractTextFromMessage(input);
    },
  });
};

/**
 * åˆ›å»º OpenAI AI å¤„ç†é“¾
 * ä½¿ç”¨ LCEL è§„èŒƒï¼šprompt.pipe(model).pipe(textExtractor)
 * æ³¨æ„ï¼šä¸ä½¿ç”¨ StringOutputParserï¼Œå› ä¸ºå®ƒæ— æ³•å¤„ç†åŒ…å«å…ƒæ•°æ®çš„æµå¼å“åº”
 * æ³¨æ„ï¼šAnthropic è¯·ä½¿ç”¨ @/services/ai/anthropic
 * @param prompt ChatPromptTemplate æç¤ºè¯æ¨¡æ¿
 * @param config æ¨¡å‹é…ç½®
 * @returns Runnable é“¾
 */
export function createAIChain<T extends Record<string, unknown>>(
  prompt: ChatPromptTemplate,
  config: AIModelConfig = {}
): Runnable<T, string> {
  const model = createOpenAIModel(config);
  const textExtractor = createTextExtractor();
  // ä½¿ç”¨è‡ªå®šä¹‰çš„æ–‡æœ¬æå–å™¨ï¼Œè€Œä¸æ˜¯ StringOutputParser
  // è¿™æ ·å¯ä»¥æ­£ç¡®å¤„ç†åŒ…å«å…ƒæ•°æ®çš„æµå¼å“åº”
  return prompt.pipe(model).pipe(textExtractor) as Runnable<T, string>;
}